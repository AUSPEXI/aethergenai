# Databricks notebook source
# COMMAND ----------

"""
04_benchmarks_evidence.py â€” Compute utility metrics and simple privacy proxies; write evidence.

Widgets:
- catalog_name, schema_name, dataset_name
- volume_uri: where to write evidence (e.g., /Volumes/aethergen/automotive/automotive_assets)
- auc_min: minimum AUC to pass gate (default 0.75)
"""

# COMMAND ----------

from pyspark.sql import functions as F
import json, math
try:
  from pyspark.ml.feature import VectorAssembler
  from pyspark.ml.classification import LogisticRegression
  from pyspark.ml.evaluation import BinaryClassificationEvaluator
  _ML_OK = True
except Exception:
  _ML_OK = False

dbutils.widgets.text("catalog_name", "aethergen", "catalog_name")
dbutils.widgets.text("schema_name", "automotive", "schema_name")
dbutils.widgets.text("dataset_name", "material_defect_v1", "dataset_name")
dbutils.widgets.text("volume_uri", "", "volume_uri")
dbutils.widgets.text("auc_min", "0.75", "auc_min")

catalog = dbutils.widgets.get("catalog_name").strip()
schema = dbutils.widgets.get("schema_name").strip()
dataset = dbutils.widgets.get("dataset_name").strip()
volume_uri = dbutils.widgets.get("volume_uri").strip().rstrip("/")
auc_min = float(dbutils.widgets.get("auc_min").strip() or "0.75")

# Ensure Spark session exists (defensive)
try:
  spark  # type: ignore
except NameError:
  from pyspark.sql import SparkSession  # type: ignore
  spark = SparkSession.builder.getOrCreate()

full_table = f"{catalog}.{schema}.{dataset}"
df = spark.table(full_table)

features = ["surface_roughness","scratch_length_mm","dent_depth_mm","temperature_c","humidity_pct"]
auc = None
ml_used = False
if _ML_OK:
  try:
    va = VectorAssembler(inputCols=features, outputCol="features")
    df2 = df.withColumn("label", F.col("defect_label").cast("double"))
    splits = df2.randomSplit([0.8, 0.2], seed=42)
    train = va.transform(splits[0]).select("features","label")
    test = va.transform(splits[1]).select("features","label")
    lr = LogisticRegression(maxIter=50)
    model = lr.fit(train)
    pred = model.transform(test)
    auc = __import__('pyspark.ml.evaluation', fromlist=['BinaryClassificationEvaluator']).BinaryClassificationEvaluator(metricName="areaUnderROC").evaluate(pred)  # lazy load
    ml_used = True
  except Exception:
    ml_used = False

# Fallback scoring and KS approximation when ML is unavailable
ks_stat = None
if not ml_used:
  scored = df.select(
    (F.col("surface_roughness")*F.lit(2.0) + F.col("scratch_length_mm") + F.col("dent_depth_mm")*F.lit(1.5)).alias("score"),
    F.col("defect_label").cast("int").alias("label")
  )
  # Approximate KS using quantiles
  qs = [i/20.0 for i in range(1,20)]
  qvals = scored.approxQuantile("score", qs, 1e-3)
  ks = 0.0
  for q in qvals:
    cdf1 = scored.filter((F.col("label") == 1) & (F.col("score") <= q)).count()
    n1 = scored.filter(F.col("label") == 1).count() or 1
    cdf0 = scored.filter((F.col("label") == 0) & (F.col("score") <= q)).count()
    n0 = scored.filter(F.col("label") == 0).count() or 1
    ks = max(ks, abs(cdf1/n1 - cdf0/n0))
  ks_stat = float(ks)

# Simple privacy proxies
# k-anonymity proxy: distinct combinations frequency >= k
k = 5
qids = ["line_id","station_id","camera_id"]
freqs = df.groupBy(qids).count()
k_proxy_violations = freqs.filter(F.col("count") < k).count()

evidence = {
  "bundle_version": "1.0",
  "dataset": full_table,
  "metrics": {
    "utility": { "auc": auc, "ks": ks_stat },
    "privacy": { "k_proxy_violations": int(k_proxy_violations), "k": k }
  },
  "ablations": {
    "features": features,
    "notes": "Feature family toggles can be evaluated in extended runs.",
    "ml_used": ml_used
  }
}

print({"auc": auc, "k_proxy_violations": int(k_proxy_violations)})

if volume_uri:
  out_path = f"{volume_uri}/{dataset}_evidence.json"
  dbutils.fs.put(out_path, json.dumps(evidence, indent=2), True)
  print({"evidence_uri": out_path})

passed = ((auc is None) or (auc >= auc_min)) and (k_proxy_violations == 0)
print({"gate_passed": passed})


