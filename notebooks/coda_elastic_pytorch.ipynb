{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CODA + Elastic Transfer (PyTorch AE Demo)\n",
        "\n",
        "This notebook mirrors the Model Lab flow with a small PyTorch AutoEncoder:\n",
        "- CODA: applies per-step learning-rate scaling and per-sample weights\n",
        "- Elastic Transfer: LoRA-style low-rank delta between trials (function-preserving warm start)\n",
        "- Energy Ledger: records collisions/allocations for evidence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, json, math, random, time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device)\n",
        "\n",
        "# Synthetic dataset (similar to claims features)\n",
        "N = 20000\n",
        "rng = np.random.default_rng(42)\n",
        "submitted = rng.normal(800, 150, N)\n",
        "allowed   = submitted * rng.uniform(0.6, 0.9, N)\n",
        "paid      = allowed * rng.uniform(0.6, 0.95, N)\n",
        "lag       = rng.integers(0, 20, N)\n",
        "X = np.stack([submitted/1000, allowed/1000, paid/1000, lag/20], axis=1).astype(np.float32)\n",
        "X_t = torch.tensor(X)\n",
        "\n",
        "# AE model\n",
        "class AE(nn.Module):\n",
        "    def __init__(self, d=4, h1=16, z=3, h2=16):\n",
        "        super().__init__()\n",
        "        self.enc = nn.Sequential(nn.Linear(d, h1), nn.ReLU(), nn.Linear(h1, z), nn.ReLU())\n",
        "        self.dec = nn.Sequential(nn.Linear(z, h2), nn.ReLU(), nn.Linear(h2, d))\n",
        "    def forward(self, x):\n",
        "        z = self.enc(x)\n",
        "        return self.dec(z)\n",
        "\n",
        "mse = nn.MSELoss(reduction='none')\n",
        "\n",
        "# CODA signals (demo)\n",
        "def estimate_mass(losses):\n",
        "    v = losses.var().item() if isinstance(losses, torch.Tensor) else np.var(losses)\n",
        "    base = 1.0 / (math.sqrt(v) + 1e-6)\n",
        "    mean = losses.mean().item() if isinstance(losses, torch.Tensor) else float(np.mean(losses))\n",
        "    if isinstance(losses, torch.Tensor): losses = losses.detach().cpu().numpy()\n",
        "    return base / (np.abs(losses - mean) + 1e-6)\n",
        "\n",
        "def coda_step(window_energy, mass, info_gain):\n",
        "    target_v = info_gain / (mass + 1e-6)\n",
        "    scale = window_energy / (target_v.sum() + 1e-8)\n",
        "    v = target_v * scale\n",
        "    lr_scale = v / (np.mean(v) + 1e-9)\n",
        "    sample_weight = info_gain / (info_gain.max() + 1e-9)\n",
        "    return lr_scale.astype(np.float32), sample_weight.astype(np.float32)\n",
        "\n",
        "# Elastic transfer (LoRA-like delta)\n",
        "def lora_delta(W, rank=2, scale=0.01):\n",
        "    A = torch.randn(W.shape[0], rank, device=W.device) * scale\n",
        "    B = torch.randn(rank, W.shape[1], device=W.device) * scale\n",
        "    return A @ B\n",
        "\n",
        "def apply_lora(model, rank=2, scale=0.01):\n",
        "    with torch.no_grad():\n",
        "        for mod in model.modules():\n",
        "            if isinstance(mod, nn.Linear):\n",
        "                mod.weight += lora_delta(mod.weight, rank, scale)\n",
        "\n",
        "# Energy ledger\n",
        "ledger = { 'entries': [], 'windowEnergy': 1.0 }\n",
        "\n",
        "def log(entry):\n",
        "    entry['time'] = time.strftime('%H:%M:%S')\n",
        "    ledger['entries'].append(entry)\n",
        "\n",
        "# Training loop with CODA + Elastic Transfer\n",
        "model = AE().to(device)\n",
        "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "steps = 300\n",
        "batch_size = 128\n",
        "\n",
        "for t in range(3):  # 3 trials to show collisions\n",
        "    # Elastic collision: warm start next trial with LoRA delta\n",
        "    if t > 0:\n",
        "        apply_lora(model, rank=4, scale=0.005)\n",
        "        log({'type':'collision', 'from': f'trial{t-1}', 'to': f'trial{t}', 'deltaEnergy': 0})\n",
        "\n",
        "    # compute initial per-sample losses for CODA mass/IG\n",
        "    with torch.no_grad():\n",
        "        out0 = model(X_t.to(device))\n",
        "        losses0 = mse(out0, X_t.to(device)).mean(dim=1).cpu().numpy()\n",
        "    mass = estimate_mass(losses0)\n",
        "    info_gain = np.maximum(1e-3, losses0)\n",
        "    lr_scale, sample_weight = coda_step(1.0, mass, info_gain)\n",
        "\n",
        "    sampler = WeightedRandomSampler(sample_weight, num_samples=len(sample_weight), replacement=True)\n",
        "    dl = DataLoader(TensorDataset(X_t), batch_size=batch_size, sampler=sampler)\n",
        "\n",
        "    for step, (xb,) in enumerate(dl):\n",
        "        if step >= steps: break\n",
        "        scale = float(lr_scale[step % len(lr_scale)])\n",
        "        for g in opt.param_groups:\n",
        "            g['lr'] = 1e-3 * scale\n",
        "        xb = xb.to(device)\n",
        "        opt.zero_grad()\n",
        "        xhat = model(xb)\n",
        "        loss = mse(xhat, xb).mean()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        opt.step()\n",
        "        if step % 50 == 0:\n",
        "            log({'type':'allocation', 'deltaEnergy': 1.0, 'details': {'lrScale': round(scale,3), 'trial': t, 'step': step, 'loss': float(loss.item())}})\n",
        "\n",
        "print('Done. Ledger entries:', len(ledger['entries']))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save ledger to JSON (local or DBFS if mounted)\n",
        "import json, os\n",
        "path = os.getenv('LEDGER_JSON_PATH', 'energy_ledger.json')\n",
        "with open(path, 'w') as f:\n",
        "    json.dump(ledger, f, indent=2)\n",
        "print('Saved ledger to', path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: write ledger to Delta if Spark is available\n",
        "try:\n",
        "    spark  # type: ignore\n",
        "    from pyspark.sql import Row\n",
        "    rows = []\n",
        "    for e in ledger['entries']:\n",
        "        rows.append(Row(time=e.get('time'), type=e.get('type'), from_=e.get('from'), to=e.get('to'), deltaEnergy=float(e.get('deltaEnergy', 0.0)), details=json.dumps(e.get('details', {}))))\n",
        "    df = spark.createDataFrame(rows)\n",
        "    spark.sql(\"CREATE DATABASE IF NOT EXISTS aethergen\")\n",
        "    df.write.mode('overwrite').format('delta').saveAsTable('aethergen.energy_ledger')\n",
        "    print('Delta table written: aethergen.energy_ledger')\n",
        "except NameError:\n",
        "    print('Spark not available; skipped Delta write')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
