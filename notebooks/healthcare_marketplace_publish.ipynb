{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Publish Healthcare Fraud Dataset to Databricks Marketplace\n",
        "Uses environment variables for workspace URL and PAT. Creates a Delta table and prepares listing payload.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, json\n",
        "import pandas as pd\n",
        "\n",
        "# df should be available (generated or loaded). Example:\n",
        "# from healthcare_fraud_generate import gen_claims\n",
        "# df = gen_claims(200_000, fraud_rate=0.03)\n",
        "\n",
        "workspace_api = os.getenv('VITE_DATABRICKS_API_URL')\n",
        "pat = os.getenv('VITE_DATABRICKS_API_KEY')\n",
        "print('Workspace API:', workspace_api)\n",
        "print('PAT configured:', bool(pat))\n",
        "\n",
        "listing = {\n",
        "    'dataset_name': 'AethergenAI â€“ Healthcare Claims Fraud v1',\n",
        "    'version': '1.0.0',\n",
        "    'provider': 'AUSPEXI',\n",
        "    'description': 'Synthetic healthcare claims with injected fraud patterns (3% prevalence).',\n",
        "    'format': 'delta_table',\n",
        "    'partitions': ['year','month'],\n",
        "    'target_table': 'healthcare_synth_v1',\n",
        "}\n",
        "print(json.dumps(listing, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare SQL to create share and add table (run on Databricks SQL)\n",
        "sql_commands = f'''\n",
        "CREATE SHARE IF NOT EXISTS aethergen_healthcare_v1;\n",
        "ALTER SHARE aethergen_healthcare_v1 ADD TABLE aethergen.healthcare_synth_v1;\n",
        "-- GRANT SELECT ON SHARE aethergen_healthcare_v1 TO RECIPIENT <your_recipient_name>;\n",
        "'''\n",
        "print(sql_commands)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Automate share + table add via Databricks REST (SQL Statement Execution API)\n",
        "import os, time, requests, json\n",
        "\n",
        "api_base = os.getenv('VITE_DATABRICKS_API_URL')  # e.g., https://.../api/2.0\n",
        "pat = os.getenv('VITE_DATABRICKS_API_KEY')\n",
        "assert api_base and pat, 'Set VITE_DATABRICKS_API_URL and VITE_DATABRICKS_API_KEY'\n",
        "\n",
        "headers = { 'Authorization': f'Bearer {pat}', 'Content-Type': 'application/json' }\n",
        "\n",
        "# 1) Pick a running SQL warehouse\n",
        "wh_list = requests.get(f\"{api_base}/sql/warehouses\", headers=headers).json()\n",
        "warehouses = wh_list.get('warehouses', [])\n",
        "if not warehouses:\n",
        "    raise RuntimeError('No SQL Warehouses found. Create/Start a Serverless Starter Warehouse and re-run.')\n",
        "\n",
        "# Prefer RUNNING, else first\n",
        "warehouse = next((w for w in warehouses if w.get('state')=='RUNNING'), warehouses[0])\n",
        "warehouse_id = warehouse['id']\n",
        "print('Using warehouse:', warehouse.get('name'), warehouse_id, warehouse.get('state'))\n",
        "\n",
        "# 2) Helper to run SQL\n",
        "\n",
        "def run_sql(sql):\n",
        "    payload = { 'statement': sql, 'warehouse_id': warehouse_id }\n",
        "    resp = requests.post(f\"{api_base}/sql/statements\", headers=headers, data=json.dumps(payload))\n",
        "    resp.raise_for_status()\n",
        "    sid = resp.json()['statement_id']\n",
        "    # poll\n",
        "    while True:\n",
        "        q = requests.get(f\"{api_base}/sql/statements/{sid}\", headers=headers)\n",
        "        q.raise_for_status()\n",
        "        s = q.json()\n",
        "        if s['status']['state'] in ('SUCCEEDED','FAILED','CANCELED'):\n",
        "            return s\n",
        "        time.sleep(1.0)\n",
        "\n",
        "# 3) SQL for catalog/schema and share\n",
        "fully_qualified_table = 'hive_metastore.aethergen.healthcare_synth_v1'\n",
        "sqls = [\n",
        "    \"CREATE SCHEMA IF NOT EXISTS hive_metastore.aethergen\",\n",
        "    f\"ALTER TABLE {fully_qualified_table} SET TBLPROPERTIES(delta.autoOptimize.optimizeWrite = true)\",\n",
        "    \"CREATE SHARE IF NOT EXISTS aethergen_healthcare_v1\",\n",
        "    f\"ALTER SHARE aethergen_healthcare_v1 ADD TABLE {fully_qualified_table}\"\n",
        "]\n",
        "\n",
        "results = []\n",
        "for s in sqls:\n",
        "    print('Executing:', s)\n",
        "    r = run_sql(s)\n",
        "    print('  ->', r['status']['state'])\n",
        "    results.append((s, r['status']['state']))\n",
        "\n",
        "print('Done. Review share aethergen_healthcare_v1 and create a Recipient to grant access.')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
