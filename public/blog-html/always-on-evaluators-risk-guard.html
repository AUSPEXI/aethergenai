<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Always‑on Evaluators: Cheap, Continuous Risk Scoring for Reliable AI</title>
  <meta name="description" content="Compact SLM evaluators score toxicity, PII, prompt injection, bias, and jailbreaking per turn. Scores feed Risk Guard to generate, fetch context, reroute, or fail‑close—cutting tokens, latency, and energy." />
  <style>
    body { font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial; color:#0f172a; line-height:1.6; }
    .container { max-width: 760px; margin: 40px auto; padding: 0 16px; }
    h1 { font-size: 2rem; font-weight: 800; color:#0f172a; margin-bottom: 12px; }
    h2 { font-size: 1.25rem; font-weight: 700; color:#0f172a; margin-top: 28px; }
    p { color:#1f2937; }
    .note { background:#ecfeff; border:1px solid #a5f3fc; padding:12px 14px; border-radius:10px; color:#0c4a6e; }
    .kbd { display:inline-block; padding:.1rem .35rem; font-size:.875rem; border:1px solid #cbd5e1; border-bottom-width:2px; border-radius:.375rem; background:white; }
    a { color:#2563eb; text-decoration: none; }
    a:hover { text-decoration: underline; }
    ul { margin-left: 1.25rem; }
  </style>
</head>
<body>
  <div class="container">
    <h1>Always‑on Evaluators: Cheap, Continuous Risk Scoring for Reliable AI</h1>
    <p class="note">Summary: Compact small‑language‑model evaluators run per‑turn to score metrics like toxicity, PII, prompt injection, bias, and jailbreaking. Scores feed a pre‑generation Risk Guard that decides whether to generate, fetch more context, reroute, or abstain (fail‑closed). Result: fewer tokens and calls, lower latency and energy, and auditable behavior.</p>

    <h2>Why evaluators?</h2>
    <p>Generative systems fail in the gaps: thin context, adversarial prompts, or unclear objectives. Instead of hoping, we measure. A set of compact evaluators score each turn and enforce policy before we generate. This is inexpensive enough to run continuously on CPU/NPU.</p>

    <h2>What we score</h2>
    <ul>
      <li>Prompt injection, PII leakage, tool‑error patterns</li>
      <li>Toxicity, bias, jailbreak attempts</li>
      <li>Optional goal‑completion heuristics for tool‑using agents</li>
    </ul>

    <h2>How it integrates</h2>
    <ul>
      <li><b>Signals → Risk Guard:</b> evaluator scores are inputs to our pre‑generation gate alongside retrieval support, margin, and entropy.</li>
      <li><b>Actions:</b> below threshold → generate; above → fetch more context; well above → abstain or reroute.</li>
      <li><b>Evidence:</b> we log <code class="kbd">evaluation_events.json</code> and an aggregated <code class="kbd">evaluation_summary.json</code> into the signed evidence bundle.</li>
      <li><b>Runtime:</b> CPU/NPU‑first via our runner; GPU optional. Cheap metrics parallelised; heavier ones sampled.</li>
    </ul>

    <h2>Controls</h2>
    <ul>
      <li>Per‑metric thresholds with a fail‑closed policy</li>
      <li>Segment‑aware calibration (e.g., by product, region)</li>
      <li>Live toggle in the Stability Demo: “Always‑on Evaluators (20 metrics)”</li>
    </ul>

    <h2>Impact</h2>
    <ul>
      <li>Token savings via early abstention and rerouting</li>
      <li>Latency reduction by avoiding unnecessary large‑model calls</li>
      <li>Energy and cost reduction; on‑device friendly</li>
      <li>Auditability through signed evidence</li>
    </ul>

    <p>Read next: <a href="/blog/evidence-efficient-ai-73-percent-faster">Evidence‑Efficient AI (73%)</a> · <a href="/blog/a-billion-queries-10-months-promise-kept">A Billion Queries</a> · <a href="/account/dashboard">Dashboard</a></p>
  </div>
</body>
</html>


