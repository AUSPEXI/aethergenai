<h1>Evidence Bundles & Testing: Trustworthy AI Without Exposing IP</h1>
<p><em>By Gwylym Owen — 18–24 min read</em></p>

<h2>Executive Summary</h2>
<p>AethergenPlatform ships <strong>evidence bundles</strong> with every model/dataset release: signed metrics, configs, seeds, and hashes that let buyers and auditors reproduce claims—without revealing proprietary internals.</p>

<h2>What We Publish</h2>
<ul>
  <li>Utility at operating points with confidence intervals.</li>
  <li>Stability across segments; drift sensitivity.</li>
  <li>Ablation tables: what changed outcomes.</li>
  <li>Limits and intended‑use statements.</li>
  <li>SBOM, artifact hashes, and lineage.</li>
  <li>Privacy probes (and DP budgets if used).</li>
 </ul>

<h2>What We Withhold</h2>
<ul>
  <li>Raw training corpora and proprietary recipe internals.</li>
  <li>Weights beyond declared formats (unless contracted).</li>
  <li>Security‑sensitive thresholds and anti‑abuse heuristics.</li>
 </ul>

<h2>Testing Matrix</h2>
<ul>
  <li>Utility at fixed budgets (alerts/day, error tolerance).</li>
  <li>Segment stability (product/region/lifecycle).</li>
  <li>Robustness (noise/corruptions) where applicable.</li>
  <li>Drift early‑warning and rollback rehearsal.</li>
 </ul>

<h2>Signing & CI</h2>
<ul>
  <li>Evidence built in CI; signed; attached to change‑control.</li>
  <li>Hashes recorded; dashboards exported to HTML/PDF.</li>
  <li>Artifacts tracked with SBOM for supply‑chain integrity.</li>
 </ul>

<h2>Case Study</h2>
<p>For a healthcare detector, we published operating‑point utility, stability across three regions, and drift monitors. Procurement accepted with a 6‑month refresh cadence and rollback SOP.</p>

<h2>FAQ</h2>
<details>
  <summary>How is this different from a slide deck?</summary>
  <p>It’s reproducible. If you rerun with the same seeds/configs, you get the same metrics within confidence intervals.</p>
 </details>
<details>
  <summary>What if regulators ask for raw data?</summary>
  <p>We provide synthetic corpora with measured fidelity/utility; for restricted cases, data stays within the customer enclave.</p>
 </details>

<h2>Glossary</h2>
<ul>
  <li><strong>Evidence bundle</strong>: signed package of metrics/configs/hashes.</li>
  <li><strong>Operating point</strong>: threshold where buyers should evaluate.</li>
  <li><strong>SBOM</strong>: software bill of materials.</li>
 </ul>

<h2>Checklist</h2>
<ul>
  <li>Operating points declared; CIs attached.</li>
  <li>Stability and drift thresholds documented.</li>
  <li>Limits and intended use spelled out.</li>
  <li>SBOM and hashes validated.</li>
 </ul>

<p><a class="aeg-btn" href="/contact">Contact Sales →</a></p>

<h2>Sample Evidence Bundle Index</h2>
<pre>
index.json
├─ metrics/
│  ├─ utility@op.json
│  ├─ stability_by_segment.json
│  ├─ drift_early_warning.json
│  └─ robustness_corruptions.json
├─ plots/
│  ├─ roc_pr_curves.html
│  ├─ operating_point_tradeoffs.html
│  └─ segment_bars.html
├─ configs/
│  ├─ evaluation.yaml
│  └─ thresholds.yaml
├─ seeds/
│  └─ seeds.txt
├─ sbom.json
└─ manifest.json
</pre>

<h2>Operating Point Examples</h2>
<ul>
  <li>Healthcare: 1% FPR → +18% cases found vs baseline, CI ±2.1%.</li>
  <li>Payments: 2,000 alerts/day cap → +11% true positives, stable weekends.</li>
  <li>Edge vision: 0.92 threshold → rework minutes −9%/1k units.</li>
</ul>

<h2>Audit Workflow</h2>
<ol>
  <li>Recompute metrics using provided configs and seeds.</li>
  <li>Check CI bands and confirm alignment with published values.</li>
  <li>Verify SBOM and artifact hashes; review change‑log.</li>
  <li>Record acceptance and attach evidence IDs to change‑control.</li>
</ol>

<h2>Procurement Questionnaire</h2>
<ul>
  <li>Which operating points are supported and why?</li>
  <li>What are segment stability results and limits?</li>
  <li>How are drift incidents detected and rolled back?</li>
  <li>What is the refresh cadence and SLA?</li>
</ul>

<h2>Template: Evidence Manifest</h2>
<pre>
{
  "version": "2025.01",
  "artifacts": {
    "metrics": ["metrics/utility@op.json", "metrics/stability_by_segment.json"],
    "plots": ["plots/roc_pr_curves.html"],
    "configs": ["configs/evaluation.yaml", "configs/thresholds.yaml"],
    "sbom": "sbom.json"
  },
  "hashes": {"metrics/utility@op.json": "..."},
  "seeds": "seeds/seeds.txt"
}
</pre>

<h2>Appendix: Metric Definitions</h2>
<ul>
  <li><strong>Utility@OP</strong>: target KPI at specified threshold with CI.</li>
  <li><strong>Stability</strong>: max performance delta across declared segments.</li>
  <li><strong>Drift EW</strong>: early‑warning index from input/outcome shifts.</li>
  <li><strong>Robustness</strong>: degradation under controlled corruptions.</li>
</ul>

<h2>Security & Privacy Notes</h2>
<ul>
  <li>Evidence contains no PHI/PII; synthetic corpora documented separately.</li>
  <li>Optional DP budgets published where applicable.</li>
  <li>Access logs and signatures recorded for all artifacts.</li>
</ul>

<h2>Worked Example (Payments)</h2>
<p>We published a mule‑ring detector bundle with operating‑point charts, parameter logs for ring size/reuse, and stability by product. Buyers reproduced metrics within bands and adopted with a quarterly refresh SLA.</p>


<h2>Release Notes Template</h2>
<pre>
Release: model‑x 2025.01
OP: fpr=1% | Utility: 0.758 [0.749,0.767]
Stability: region ≤ 0.03 | product ≤ 0.02
Latency: p95 97ms | p99 142ms
Privacy: membership_advantage 0.03
Manifest: 8e7...
</pre>

<h2>Evidence Dashboard Sections</h2>
<ul>
  <li>Executive summary (OP, stability, latency, privacy).</li>
  <li>Segment tables with CIs and deltas.</li>
  <li>Trade‑off curves and ablation forest plots.</li>
  <li>Appendices: configs, hashes, environment.</li>
</ul>

<h2>Bundle Directory Layout</h2>
<pre>
release_2025_01/
├─ metrics/
├─ plots/
├─ configs/
├─ privacy/
├─ sbom.json
├─ manifest.json
├─ evidence.sig
└─ README.html
</pre>

<h2>Red‑Team Playbook (Evidence)</h2>
<ol>
  <li>Attempt to promote with failing stability → gate blocks; incident opened.</li>
  <li>Remove plots → packaging gate fails; CI halts.</li>
  <li>Alter OP mid‑pipeline → config hash mismatch; stop.</li>
</ol>

<h2>Buyer Quick‑Verify</h2>
<ul>
  <li>Open dashboards offline; check OP/stability summary.</li>
  <li>Verify manifest hashes and signatures.</li>
  <li>Record acceptance with bundle ID and date.</li>
 </ul>

<h2>Procurement Mapping</h2>
<ul>
  <li>Bundle → contract exhibit; SBOM → supply chain annex.</li>
  <li>SLAs → response/refresh windows tied to OP/stability.</li>
  <li>Evidence IDs included in POs/renewals.</li>
 </ul>

<h2>Compliance Matrix (Illustrative)</h2>
<pre>
jurisdiction, domain, requirement, evidence
EU, healthcare, GDPR+NIS2, privacy_probes+dp+offline_dash
US, finance, OCC/FRB SR 11‑7, op_utility+stability+change_control
UK, public, OFFICIAL-secure, airgapped_bundles+signatures+sbom
</pre>

<h2>Sample Metrics Files</h2>
<pre>
metrics/utility@op.json
{"op":"fpr=0.01","global":0.758,"ci":[0.749,0.767]}

metrics/stability_by_segment.json
{"region":{"NA":0.761,"EU":0.753,"APAC":0.749}}
</pre>

<h2>Environment Fingerprints</h2>
<pre>
{"python":"3.11.6","cuda":"12.1","libraries":{"numpy":"1.26.4","pandas":"2.2.1"}}
</pre>

<h2>Incident Template</h2>
<pre>
id: INC‑2025‑0012
summary: stability breach in APAC
action: rollback to bundle 8e7...
followup: overlay adjust; re‑evaluate; promote 2025.02
</pre>

<h2>Gates Catalog (Extended)</h2>
<ul>
  <li>Coverage: required features present; ranges respected.</li>
  <li>Utility: OP KPI ≥ target with CI.</li>
  <li>Stability: max deltas ≤ bands.</li>
  <li>Latency: p95/p99 ≤ SLOs.</li>
  <li>Privacy: probes ≤ thresholds; DP budgets documented (if used).</li>
  <li>Packaging: dashboards, sbom, manifest present and signed.</li>
 </ul>

<h2>Waiver Policy (If Ever Needed)</h2>
<ul>
  <li>Explicit approval with compensating controls.</li>
  <li>Expiry date; tracked in change‑control.</li>
 </ul>

<h2>Operational Dashboards</h2>
<ul>
  <li>Adoption and usage (queries, users, segments).</li>
  <li>OP utility and stability over time.</li>
  <li>Incident tracker and response times.</li>
 </ul>

<h2>Extended Buyers’ FAQ</h2>
<details>
  <summary>Can we re‑plot charts?</summary>
  <p>Yes—CSV/JSON metrics are bundled for reproducible plotting.</p>
 </details>
<details>
  <summary>Do bundles include explainability?</summary>
  <p>We can include model cards and attribution summaries when relevant.</p>
 </details>
<details>
  <summary>What’s the refresh cadence?</summary>
  <p>Declared per listing/project; commonly monthly or on material change.</p>
 </details>

<h2>Appendix: Commands</h2>
<pre>
sha256sum -c manifest.sha256 | cat
gpg --verify evidence.sig evidence.tar
</pre>

<h2>Appendix: README Excerpt</h2>
<pre>
How to review: verify → read dashboards → decide → file SBOM/manifest.
</pre>

<h2>Closing (Comprehensive)</h2>
<p>When evidence is part of the product, buyers don’t need persuasion; they need verification. <strong>AethergenPlatform</strong> turns every release into a verifiable unit of trust—signed, reproducible, and ready for audit.</p>

<h2>CI/CD Stages</h2>
<pre>
stage evaluate:
  run utility@op
  run stability
  run drift_early_warning
  run robustness (if applicable)
  run latency

stage evidence:
  export dashboards
  write manifest.json
  sign artifacts

stage gates:
  fail if any threshold breached

stage package:
  tar models+configs+evidence
</pre>

<h2>Acceptance Form (Template)</h2>
<pre>
bundle_id: 8e7...
op_utility: PASS
stability: PASS
latency: PASS
privacy: PASS
decision: APPROVE | REJECT
signoff: ____________  date: ________
</pre>

<h2>Latencies (Example)</h2>
<pre>
{"p50": 42, "p95": 97, "p99": 142}
</pre>

<h2>Privacy Probes (Example)</h2>
<pre>
{"membership_advantage": 0.03, "ci": [0.01, 0.05], "threshold": 0.05}
</pre>

<h2>Evidence Dashboard Contents</h2>
<ul>
  <li>OP utility with CIs and segment tables.</li>
  <li>Stability deltas and rolling windows.</li>
  <li>Latency distributions; p50/p95/p99.</li>
  <li>Privacy probe summaries and thresholds.</li>
</ul>

<h2>Legal Annex Checklist</h2>
<ul>
  <li>License scope and redistribution terms.</li>
  <li>SBOM and vulnerability reports.</li>
  <li>Export control statement (if applicable).</li>
  <li>Manifest hashes and signatures.</li>
</ul>

<h2>Buyers’ FAQ (Extended)</h2>
<details>
  <summary>How do we verify artifacts offline?</summary>
  <p>Dashboards are bundled (HTML/PDF); manifests include hashes and signatures for offline verification.</p>
 </details>
<details>
  <summary>Can we request a refresh?</summary>
  <p>Yes—CI regenerates evidence; bundle IDs increment; change‑log references the update.</p>
 </details>
<details>
  <summary>How are DP budgets chosen?</summary>
  <p>Per policy; we disclose ε, δ and expected utility impact, with probe results to validate.</p>
 </details>

<h2>Appendix: CSV/JSON Schemas</h2>
<pre>
utility@op.csv: segment:string,metric:float,ci_low:float,ci_high:float,delta:float
latency.json: {"p50":int,"p95":int,"p99":int}
</pre>

<h2>Closing (Extended)</h2>
<p>Evidence bundles turn claims into contracts. With <strong>AethergenPlatform</strong>, every release carries the artifacts that buyers can file—so adoption is faster, audits are smoother, and operations are safer.</p>

