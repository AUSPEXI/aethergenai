{
  "slug": "innovation-test-harness",
  "title": "Innovation Test Harness: Fast, Deterministic Model Evaluation",
  "summary": "Deterministic, repeatable model evaluation with ablations, stability probes, and exportable evidence.",
  "tags": ["benchmarks", "ablations", "evaluation"],
  "bodyMd": "## Why a Harness?\n\nEngineering velocity dies when each evaluation is bespoke. The harness provides **determinism** (fixed seeds, frozen recipes) and **coverage** (utility, stability, privacy) in a single, repeatable run. It produces signed evidence suitable for internal review and external buyers without exposing proprietary code.\n\n## Architecture\n- **Deterministic runs**: fixed seeds, pinned container/tooling versions, dataset snapshots (Delta commit/version).\n- **Reproducible configs**: JSON/YAML recipes for datasets, models, metrics, and probes; recipe hash is recorded.\n- **Ablations & probes**: module toggles, feature families, DP budgets; stability under noise/fault injections.\n- **Evidence export**: signed manifest with utility, privacy, and ablation deltas; checksums + SBOM for artifacts.\n\n## Capabilities\n- **Model registry**: Hugging Face + proprietary adapters with semantic versioning.\n- **Task adapters**: classification, ranking, regression, and generative scoring (BLEU/ROUGE/BERTScore).\n- **Ablations**: feature drops, module off/on, ε schedules (e.g., {0.5, 0.8, 1.0}), quantisation levels.\n- **Stability**: jitter/dropout, domain shifts, corruption (vision/text), distribution drift sweeps.\n- **Privacy**: MIA/attribute disclosure probes; optional DP training/evaluation budgets.\n- **Exports**: CSV/Parquet summaries, JSON evidence bundle, and signed manifest.json.\n\n## Workflow\n1. Select dataset/task (snapshot) and one or more models.\n2. Choose ablations and probes (e.g., ε ∈ {0.5, 0.8, 1.0}; brightness ±15%).\n3. Run the suite; compare lift vs baseline and probe outcomes.\n4. Export the evidence bundle; attach to review, contract, or listing.\n\n### Example Config\n```json\n{\n  \"task\": \"classification\",\n  \"dataset\": { \"name\": \"aml_graph_v2\", \"deltaVersion\": 143 },\n  \"models\": [\n    { \"id\": \"hf:distilroberta-base\", \"rev\": \"1.1.0\" },\n    { \"id\": \"internal:graph-ranker-v3\", \"rev\": \"3.2.4\" }\n  ],\n  \"ablations\": { \"features\": [\"device_id\"], \"dp_epsilons\": [0.5, 0.8, 1.0] },\n  \"stability\": { \"packet_drop\": 0.1, \"time_shift\": \"7d\" },\n  \"metrics\": [\"auc\", \"f1\", \"ks\"],\n  \"privacy\": { \"mia\": true, \"attribute\": [\"age_band\", \"segment\"] }\n}\n```\n\n## Evidence Outputs\n- **Utility**: AUC/F1/MAE/KS deltas vs baseline; confidence intervals where applicable.\n- **Stability**: performance under jitter/dropout/corruption; drift sensitivity over time.\n- **Privacy**: MIA/attribute disclosure summaries; DP ε/δ and training knobs if used.\n- **Lineage**: dataset versions, recipe hash, artifact checksums, container digest.\n- **Ablation table**: per‑toggle deltas and interactions for review.\n\n## Reading Results\n- Changes that matter are highlighted with effect sizes and uncertainty.\n- Failsafe: if privacy bounds exceed thresholds or utility regresses beyond tolerance, the run is marked **fail‑closed**.\n\n## Example Scenarios\n- **AML**: rankers under typology injections and DP budgets; optimise for false‑positive reduction at fixed recall.\n- **QC Vision**: throughput vs sensitivity as lighting/vibration vary; acceptance computed per defect class.\n- **LLM**: prompt templates and guardrails; toxicity/hallucination guard metrics tracked with scorecards.\n\n## Platform Tie‑In\nUse the Benchmarks module to define the run, then export via Reporting. The bundle includes metrics.json, privacy.json, ablations.csv, lineage.json, and manifest.json with signatures so reviewers can trust, reproduce, or escalate confidently."
}


