<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Privacy in Practice: Probes, Budgets, and Measurable Boundaries</title>
<link rel="canonical" href="https://auspexi.com/blog/privacy-in-practice-probes-budgets-measurable-boundaries"/>
<meta name="description" content="By Gwylym Owen — 30–45 min read"/>
<script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Privacy in Practice: Probes, Budgets, and Measurable Boundaries","author":{"@type":"Person","name":"Gwylym Pryce-Owen"},"mainEntityOfPage":"https://auspexi.com/blog/privacy-in-practice-probes-budgets-measurable-boundaries","datePublished":"2025-09-06T17:18:32.479Z","dateModified":"2025-09-06T17:18:32.479Z","image":"https://auspexi.com/og-image.svg","publisher":{"@type":"Organization","name":"Auspexi"},"license":"PROPRIETARY","creator":{"@type":"Organization","name":"Auspexi"},"description":"By Gwylym Owen — 30–45 min read","articleBody":"Privacy in Practice: Probes, Budgets, and Measurable Boundaries By Gwylym Owen — 30–45 min read Executive Summary Privacy is a measurement . AethergenPlatform provides privacy probes , optional differential privacy budgets, and evidence for verification. No hype—just boundaries you and your regulators can trust as of September 2025. Threats & Goals Threats to address: Membership Inference : Detect whether a record influenced training. Attribute Disclosure : Predict sensitive attributes above baseline leakage. Linkage : Re-identify synthetic records via matching. Our goal? Keep privacy rock-solid while letting your models shine. Probes: The Privacy Sniff Test These are our spy tools to catch any leaks—here’s how we roll: Train Attack Models : Pit a sneaky model against real vs. synthetic data to measure membership advantage over random guessing (e.g., AUC boost of 0.03). Predict Sensitive Attributes : Guess stuff like age or diagnosis, then compare leakage against a baseline (e.g., 0.02 above random). Report CIs & Thresholds : Give you confidence intervals (e.g., [0.01, 0.05]) and pass/fail gates (e.g., ≤ 0.05) per your policy. Differential Privacy (Optional) Program elements: Per-Release Budgets : Set ε (e.g., 2.0) and δ (e.g., 1e-6) to limit a record’s impact, tracked across releases. Utility Impact : Show how it affects operating points (e.g., -1% accuracy at 1% FPR) so you know the trade-off. Evidence Bundle : Pack in DP parameters, summaries, and a little “trust me” note for the auditors! Process Controls Controls: Seed Minimisation : Keep random seeds locked down to avoid peeking. Isolation from Artifacts : Evaluation data stays separate—no sneaky cross-contamination! Access Control, Logging, Retention : Who sees what, when, and for how long (e.g., 365 days)—all logged and signed. Review and Sign-Off : Tie it to evidence for that final thumbs-up. Reporting Template membership_advantage: 0.03 (ci [0.01,0.05]) threshold Use Case Example: Healthcare Scenario : A healthcare team shipped a patient corpus with documented privacy controls. Setup : Probes ran, DP was optional, and evidence was stacked. Result : Membership advantage clocked in at 0.02 (below 0.05), utility held strong at OP, and auditors nodded. Win : Procurement signed off with a 6‑month refresh policy. Use Case Example: Finance Scenario : A bank fortified its fraud models offline. Setup : Probes tested linkage, DP set at ε=1.5, evidence bundled. Result : Attribute disclosure stayed at 0.01 (below 0.03), with a -0.5% utility hit deemed acceptable. Win : Audit passed remotely, avoiding a site visit. FAQ Do we always need DP? Nah—only if the regulators are breathing down your neck! Probes and process controls can handle it otherwise—your call! Can probes be gamed? Ha, nice try! We use multiple tricks and CIs to keep it honest—methodology’s in the evidence, so no cheating allowed! Glossary Membership Inference : That sneaky attempt to spot a record in the training crowd. DP : Differential privacy—limits the influence of a single record. Baseline Leakage : How much info leaks without the juicy stuff—our starting line! Checklist Probes Run : Check those CIs and thresholds—did we pass? DP Parameters : Documented if used—ε and δ locked in? Process Controls : Verified—seeds safe, logs signed? Evidence Manifest : Signed and stored with the release—done deal! Appendix: Probe Sketch—Geek Mode On train_attack(real, synth) → score advantage = auc(score) - 0.5 ci = bootstrap(advantage, n=1000) Appendix: Policy Snippet—Rules to Live By privacy: membership_advantage_max: 0.05 attribute_disclosure_max: 0.03 dp_optional: true Regulatory Mapping: Covering All Bases HIPAA : Strip identifiers, document de-identification, lock access, keep audit trails—done! GDPR : Lawfulness, minimisation, purpose checks; DPIA if needed, with evidence explaining decisions. PCI : No payment card numbers in our playground—keys segregated, artifacts rotated! Risk Register: What Could Go Wrong? risk, likelihood, impact, control, owner seed_leak, low, high, isolation+logging, data_custodian probe_bypass, low, medium, multi_probe+CI, privacy_lead budget_misuse, low, medium, policy+approval, governance Probe Configurations: The Toolkit Membership Inference : Shadow model vs. attack classifier—AUC advantage is our score. Attribute Disclosure : Target fields (e.g., age) with an ethics nod—compare vs. baseline. Linkage Checks : Locality-sensitive hashing on embeddings—thresholded to catch sneaky links. Attack/Defense Cookbook Attack : Train a baddie on real vs. synthetic labels, test on hold-out data—let’s see ‘em try! Defense : Cut memorisation, sprinkle noise if DP’s on, and isolate processes like a vault. Measure : Drop the advantage with CI, compare to your policy—pass or fail, we’ll know! Red-Team Playbook Scenarios : Membership, attribute, linkage—set the success bar high. Run Attacks : Record evidence, laugh at the attempts, suggest fixes. Re-Run Probes : Verify thresholds—back to the drawing board if needed! Audit Pack Structure privacy_audit/ ├─ report.html ├─ probes/ │ ├─ membership.json │ ├─ attribute.json │ └─ linkage.json ├─ configs/ │ └─ probes.yaml └─ manifest.json Evidence Correlation Link Results : Tie privacy to utility@OP and stability—show the full picture. Trade-Offs : Spill the beans on any OP tweaks—transparency is king! DP Overview Budget ε : How much one record can stir the pot—lower means tighter privacy! Utility Loss : We measure the hit (e.g., -1% at OP) so you’re not guessing. δ : That tiny chance of a slip-up—kept super small, like 1 in a million! DP Application Notes When to Use : Only if policy demands it—probes can handle the rest. Document It : Log mechanisms, budgets, and composition—show your work! Impact Examples : Expect -0.5% to -2% utility with CIs—your call if it’s worth it! Operational SOP Before Release : Run probes, compile evidence, obtain sign‑off. During Release : Attach the audit pack, log manifest IDs in change-control—lock it down! After Release : Watch for hiccups, schedule refreshes, or rotate if needed—keep it smooth! Procurement Q&A Thresholds? Set by policy (e.g., 0.05 advantage)—we’ll explain why! Membership Advantage? Measured with AUC—full details in evidence! DP Used? Optional, with budgets (e.g., ε=2.0)—mechanisms disclosed! Artifact Storage? Air-gapped, access-logged—only the trusted get in! Policy Snippets (YAML): The Rulebook policy: probes: membership_advantage_max: 0.05 attribute_disclosure_max: 0.03 dp: enabled: false epsilon: 2.0 delta: 1e-6 process: seed_isolation: true logs_retention_days: 365 Monitoring Track Metrics : Watch probe trends—alert if they wobble! Log Evidence : Keep hashes immutable—no funny business! Quarterly Review : Check thresholds and policies—stay sharp! Case Notes: Public Sector For a government gig, we ran probes in an air-gapped bunker, stored audit packs locally, and only leaked summary metrics to the outside world. DP was optional, and membership advantage stayed below the line—mission accomplished! Extended FAQ: More Laughs, More Answers Are probes run on every refresh? Yes—every release includes probes bundled with evidence. Can third parties validate probes? Yep—give ‘em configs, seeds, and manifests, and let ‘em play in their own sandbox! How do probes relate to explainability? Different gigs—probes catch leaks, explainability shows the ‘why.’ Both in your evidence pack! Templates: The Blueprint probe_results.json { \"membership_advantage\": {\"value\": 0.03, \"ci\": [0.01,0.05]}, \"attribute_disclosure\": {\"value\": 0.02, \"baseline\": 0.02}, \"dp\": {\"enabled\": false} } Incident Runbook Spot the Oops : Catch a probe regression—uh-oh! Freeze & Triage : Halt releases, call the crew—time to fix! Investigate : Check seeds/process, slap on mitigations—get creative! Re-Run & Resume : Probes back on, report attached—back in business! Governance Hooks Policy IDs : In evidence manifests—trace it back! Approvals : Logged with names—accountability rocks! Privacy KPIs : In CI dashboards—watch the scoreboard! Closing Notes Privacy by measurement is how trust is earned. AethergenPlatform turns probes, budgets, and controls into a repeatable program with boundaries that teams and auditors can verify. Contact Sales →"}</script>
<style>

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: none;
            margin: 0 auto;
            padding: 20px;
            background-color: #f9f9f9;
        }
        .article {
            background: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1 {
            color: #2c3e50;
            font-size: 2.5em;
            margin-bottom: 20px;
            line-height: 1.2;
        }
        h2 {
            color: #34495e;
            font-size: 1.8em;
            margin-top: 40px;
            margin-bottom: 20px;
        }
        p {
            margin-bottom: 20px;
            font-size: 1.1em;
        }
        .meta {
            color: #7f8c8d;
            font-size: 0.9em;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid #ecf0f1;
        }
        .highlight {
            background-color: #e8f5e8;
            padding: 15px;
            border-left: 4px solid #27ae60;
            margin: 20px 0;
        }
        blockquote {
            border-left: 4px solid #3498db;
            padding-left: 20px;
            margin: 20px 0;
            font-style: italic;
            color: #555;
        }
        .framework {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            border-left: 4px solid #27ae60;
        }
        .framework h3 {
            color: #2c3e50;
            margin-bottom: 15px;
        }
        .framework ol {
            padding-left: 20px;
        }
        .framework li {
            margin-bottom: 10px;
        }
        .carbon-tracker {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 25px;
            border-radius: 12px;
            margin: 25px 0;
        }
        .carbon-tracker h3 {
            margin-bottom: 15px;
            font-size: 1.4em;
        }
        .carbon-tracker ul {
            list-style: none;
            padding: 0;
        }
        .carbon-tracker li {
            padding: 8px 0;
            border-bottom: 1px solid rgba(255,255,255,0.2);
        }
        .carbon-tracker li:last-child {
            border-bottom: none;
        }
    
  </style>
</head>
<body>

<div style="position:sticky;top:0;z-index:50;background:#ffffff;border-bottom:1px solid #e5e7eb;">
  <div style="max-width:960px;margin:0 auto;padding:10px 16px;display:flex;align-items:center;gap:12px;">
    <a href="/" style="color:#0f172a;text-decoration:none;font-weight:700">Auspexi</a>
    <button onclick="(function(){try{history.back()}catch(e){} setTimeout(function(){ if(!document.referrer || !/\/blog/.test(document.referrer)){ location.href='/blog' } },50);})()" style="margin-left:auto;background:#2563eb;color:#fff;border:none;padding:6px 10px;border-radius:6px;cursor:pointer">← Back to Blog</button>
  </div>
</div>

  <div class="article">
<h1>Privacy in Practice: Probes, Budgets, and Measurable Boundaries</h1>
  <p><em>By Gwylym Owen — 30–45 min read</em></p>

  <h2>Executive Summary</h2>
  <p>Privacy is a <strong>measurement</strong>. AethergenPlatform provides privacy <strong>probes</strong>, optional <strong>differential privacy</strong> budgets, and <strong>evidence</strong> for verification. No hype—just boundaries you and your regulators can trust as of September 2025.</p>

  <h2>Threats & Goals</h2>
  <p>Threats to address:</p>
  <ul>
    <li><strong>Membership Inference</strong>: Detect whether a record influenced training.</li>
    <li><strong>Attribute Disclosure</strong>: Predict sensitive attributes above baseline leakage.</li>
    <li><strong>Linkage</strong>: Re-identify synthetic records via matching.</li>
  </ul>
  <p>Our goal? Keep privacy rock-solid while letting your models shine.</p>

  <h2>Probes: The Privacy Sniff Test</h2>
  <p>These are our spy tools to catch any leaks—here’s how we roll:</p>
  <ul>
    <li><strong>Train Attack Models</strong>: Pit a sneaky model against real vs. synthetic data to measure membership advantage over random guessing (e.g., AUC boost of 0.03).</li>
    <li><strong>Predict Sensitive Attributes</strong>: Guess stuff like age or diagnosis, then compare leakage against a baseline (e.g., 0.02 above random).</li>
    <li><strong>Report CIs & Thresholds</strong>: Give you confidence intervals (e.g., [0.01, 0.05]) and pass/fail gates (e.g., ≤ 0.05) per your policy.</li>
  </ul>

  <h2>Differential Privacy (Optional)</h2>
  <p>Program elements:</p>
  <ul>
    <li><strong>Per-Release Budgets</strong>: Set ε (e.g., 2.0) and δ (e.g., 1e-6) to limit a record’s impact, tracked across releases.</li>
    <li><strong>Utility Impact</strong>: Show how it affects operating points (e.g., -1% accuracy at 1% FPR) so you know the trade-off.</li>
    <li><strong>Evidence Bundle</strong>: Pack in DP parameters, summaries, and a little “trust me” note for the auditors!</li>
  </ul>

  <h2>Process Controls</h2>
  <p>Controls:</p>
  <ul>
    <li><strong>Seed Minimisation</strong>: Keep random seeds locked down to avoid peeking.</li>
    <li><strong>Isolation from Artifacts</strong>: Evaluation data stays separate—no sneaky cross-contamination!</li>
    <li><strong>Access Control, Logging, Retention</strong>: Who sees what, when, and for how long (e.g., 365 days)—all logged and signed.</li>
    <li><strong>Review and Sign-Off</strong>: Tie it to evidence for that final thumbs-up.</li>
  </ul>

  <h2>Reporting Template</h2>
  <pre>
membership_advantage: 0.03 (ci [0.01,0.05]) threshold <= 0.05 PASS
attribute_disclosure: 0.02 above baseline? NO PASS
dp_budget: epsilon=2.0, delta=1e-6
  </pre>

  <h2>Use Case Example: Healthcare</h2>
  <p><strong>Scenario</strong>: A healthcare team shipped a patient corpus with documented privacy controls.</p>
  <ul>
    <li><strong>Setup</strong>: Probes ran, DP was optional, and evidence was stacked.</li>
    <li><strong>Result</strong>: Membership advantage clocked in at 0.02 (below 0.05), utility held strong at OP, and auditors nodded.</li>
    <li><strong>Win</strong>: Procurement signed off with a 6‑month refresh policy.</li>
  </ul>

  <h2>Use Case Example: Finance</h2>
  <p><strong>Scenario</strong>: A bank fortified its fraud models offline.</p>
  <ul>
    <li><strong>Setup</strong>: Probes tested linkage, DP set at ε=1.5, evidence bundled.</li>
    <li><strong>Result</strong>: Attribute disclosure stayed at 0.01 (below 0.03), with a -0.5% utility hit deemed acceptable.</li>
    <li><strong>Win</strong>: Audit passed remotely, avoiding a site visit.</li>
  </ul>

  <h2>FAQ</h2>
  <details>
    <summary>Do we always need DP?</summary>
    <p>Nah—only if the regulators are breathing down your neck! Probes and process controls can handle it otherwise—your call!</p>
  </details>
  <details>
    <summary>Can probes be gamed?</summary>
    <p>Ha, nice try! We use multiple tricks and CIs to keep it honest—methodology’s in the evidence, so no cheating allowed!</p>
  </details>

  <h2>Glossary</h2>
  <ul>
    <li><strong>Membership Inference</strong>: That sneaky attempt to spot a record in the training crowd.</li>
    <li><strong>DP</strong>: Differential privacy—limits the influence of a single record.</li>
    <li><strong>Baseline Leakage</strong>: How much info leaks without the juicy stuff—our starting line!</li>
  </ul>

  <h2>Checklist</h2>
  <ul>
    <li><strong>Probes Run</strong>: Check those CIs and thresholds—did we pass?</li>
    <li><strong>DP Parameters</strong>: Documented if used—ε and δ locked in?</li>
    <li><strong>Process Controls</strong>: Verified—seeds safe, logs signed?</li>
    <li><strong>Evidence Manifest</strong>: Signed and stored with the release—done deal!</li>
  </ul>

  <h2>Appendix: Probe Sketch—Geek Mode On</h2>
  <pre>
train_attack(real, synth) → score
advantage = auc(score) - 0.5
ci = bootstrap(advantage, n=1000)
  </pre>

  <h2>Appendix: Policy Snippet—Rules to Live By</h2>
  <pre>
privacy:
  membership_advantage_max: 0.05
  attribute_disclosure_max: 0.03
  dp_optional: true
  </pre>

  <h2>Regulatory Mapping: Covering All Bases</h2>
  <ul>
    <li><strong>HIPAA</strong>: Strip identifiers, document de-identification, lock access, keep audit trails—done!</li>
    <li><strong>GDPR</strong>: Lawfulness, minimisation, purpose checks; DPIA if needed, with evidence explaining decisions.</li>
    <li><strong>PCI</strong>: No payment card numbers in our playground—keys segregated, artifacts rotated!</li>
  </ul>

  <h2>Risk Register: What Could Go Wrong?</h2>
  <pre>
risk, likelihood, impact, control, owner
seed_leak, low, high, isolation+logging, data_custodian
probe_bypass, low, medium, multi_probe+CI, privacy_lead
budget_misuse, low, medium, policy+approval, governance
  </pre>

  <h2>Probe Configurations: The Toolkit</h2>
  <ul>
    <li><strong>Membership Inference</strong>: Shadow model vs. attack classifier—AUC advantage is our score.</li>
    <li><strong>Attribute Disclosure</strong>: Target fields (e.g., age) with an ethics nod—compare vs. baseline.</li>
    <li><strong>Linkage Checks</strong>: Locality-sensitive hashing on embeddings—thresholded to catch sneaky links.</li>
  </ul>

  <h2>Attack/Defense Cookbook</h2>
  <ul>
    <li><strong>Attack</strong>: Train a baddie on real vs. synthetic labels, test on hold-out data—let’s see ‘em try!</li>
    <li><strong>Defense</strong>: Cut memorisation, sprinkle noise if DP’s on, and isolate processes like a vault.</li>
    <li><strong>Measure</strong>: Drop the advantage with CI, compare to your policy—pass or fail, we’ll know!</li>
  </ul>

  <h2>Red-Team Playbook</h2>
  <ol>
    <li><strong>Scenarios</strong>: Membership, attribute, linkage—set the success bar high.</li>
    <li><strong>Run Attacks</strong>: Record evidence, laugh at the attempts, suggest fixes.</li>
    <li><strong>Re-Run Probes</strong>: Verify thresholds—back to the drawing board if needed!</li>
  </ol>

  <h2>Audit Pack Structure</h2>
  <pre>
privacy_audit/
├─ report.html
├─ probes/
│  ├─ membership.json
│  ├─ attribute.json
│  └─ linkage.json
├─ configs/
│  └─ probes.yaml
└─ manifest.json
  </pre>

  <h2>Evidence Correlation</h2>
  <ul>
    <li><strong>Link Results</strong>: Tie privacy to utility@OP and stability—show the full picture.</li>
    <li><strong>Trade-Offs</strong>: Spill the beans on any OP tweaks—transparency is king!</li>
  </ul>

  <h2>DP Overview</h2>
  <ul>
    <li><strong>Budget ε</strong>: How much one record can stir the pot—lower means tighter privacy!</li>
    <li><strong>Utility Loss</strong>: We measure the hit (e.g., -1% at OP) so you’re not guessing.</li>
    <li><strong>δ</strong>: That tiny chance of a slip-up—kept super small, like 1 in a million!</li>
  </ul>

  <h2>DP Application Notes</h2>
  <ul>
    <li><strong>When to Use</strong>: Only if policy demands it—probes can handle the rest.</li>
    <li><strong>Document It</strong>: Log mechanisms, budgets, and composition—show your work!</li>
    <li><strong>Impact Examples</strong>: Expect -0.5% to -2% utility with CIs—your call if it’s worth it!</li>
  </ul>

  <h2>Operational SOP</h2>
  <ol>
    <li><strong>Before Release</strong>: Run probes, compile evidence, obtain sign‑off.</li>
    <li><strong>During Release</strong>: Attach the audit pack, log manifest IDs in change-control—lock it down!</li>
    <li><strong>After Release</strong>: Watch for hiccups, schedule refreshes, or rotate if needed—keep it smooth!</li>
  </ol>

  <h2>Procurement Q&A</h2>
  <ul>
    <li><strong>Thresholds?</strong> Set by policy (e.g., 0.05 advantage)—we’ll explain why!</li>
    <li><strong>Membership Advantage?</strong> Measured with AUC—full details in evidence!</li>
    <li><strong>DP Used?</strong> Optional, with budgets (e.g., ε=2.0)—mechanisms disclosed!</li>
    <li><strong>Artifact Storage?</strong> Air-gapped, access-logged—only the trusted get in!</li>
  </ul>

  <h2>Policy Snippets (YAML): The Rulebook</h2>
  <pre>
policy:
  probes:
    membership_advantage_max: 0.05
    attribute_disclosure_max: 0.03
  dp:
    enabled: false
    epsilon: 2.0
    delta: 1e-6
  process:
    seed_isolation: true
    logs_retention_days: 365
  </pre>

  <h2>Monitoring</h2>
  <ul>
    <li><strong>Track Metrics</strong>: Watch probe trends—alert if they wobble!</li>
    <li><strong>Log Evidence</strong>: Keep hashes immutable—no funny business!</li>
    <li><strong>Quarterly Review</strong>: Check thresholds and policies—stay sharp!</li>
  </ul>

  <h2>Case Notes: Public Sector</h2>
  <p>For a government gig, we ran probes in an air-gapped bunker, stored audit packs locally, and only leaked summary metrics to the outside world. DP was optional, and membership advantage stayed below the line—mission accomplished!</p>

  <h2>Extended FAQ: More Laughs, More Answers</h2>
  <details>
    <summary>Are probes run on every refresh?</summary>
    <p>Yes—every release includes probes bundled with evidence.</p>
  </details>
  <details>
    <summary>Can third parties validate probes?</summary>
    <p>Yep—give ‘em configs, seeds, and manifests, and let ‘em play in their own sandbox!</p>
  </details>
  <details>
    <summary>How do probes relate to explainability?</summary>
    <p>Different gigs—probes catch leaks, explainability shows the ‘why.’ Both in your evidence pack!</p>
  </details>

  <h2>Templates: The Blueprint</h2>
  <pre>
probe_results.json
{
  "membership_advantage": {"value": 0.03, "ci": [0.01,0.05]},
  "attribute_disclosure": {"value": 0.02, "baseline": 0.02},
  "dp": {"enabled": false}
}
  </pre>

  <h2>Incident Runbook</h2>
  <ol>
    <li><strong>Spot the Oops</strong>: Catch a probe regression—uh-oh!</li>
    <li><strong>Freeze & Triage</strong>: Halt releases, call the crew—time to fix!</li>
    <li><strong>Investigate</strong>: Check seeds/process, slap on mitigations—get creative!</li>
    <li><strong>Re-Run & Resume</strong>: Probes back on, report attached—back in business!</li>
  </ol>

  <h2>Governance Hooks</h2>
  <ul>
    <li><strong>Policy IDs</strong>: In evidence manifests—trace it back!</li>
    <li><strong>Approvals</strong>: Logged with names—accountability rocks!</li>
    <li><strong>Privacy KPIs</strong>: In CI dashboards—watch the scoreboard!</li>
  </ul>

  <h2>Closing Notes</h2>
  <p>Privacy by measurement is how trust is earned. <strong>AethergenPlatform</strong> turns probes, budgets, and controls into a repeatable program with boundaries that teams and auditors can verify.</p>

  <p><a href="/contact" class="aeg-btn">Contact Sales →</a></p>
  </div>

</body>
</html>
